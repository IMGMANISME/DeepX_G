{"cells":[{"cell_type":"markdown","metadata":{"id":"-gZwlZ4layy9"},"source":["#專題資料結構\n","```\n","Project: DeepX\n","└─ original_dataset\n","   ├─ abnormal\n","   │   └─ abnormal_label\n","   │   └─ abnormal(.dcm)\n","   │   └─ abnormal(.jpg)\n","   ├─ normal\n","   │   └─ normal_label\n","   │   └─ normal(.dcm)\n","   │   └─ normal(.jpg)\n","└─ yolov8_custom\n","└─ processed_dataset\n","   ├─ abnormal\n","   ├─ normal\n","└─ all_data\n","└─ splitted_dataset\n","   ├─ test\n","   │   └─ abnormal\n","   │   └─ normal\n","   ├─ train\n","   │   └─ abnormal\n","   │   └─ normal\n","   ├─ train_argumentation\n","   │   └─ abnormal\n","   │   └─ normal\n","└─ tensor\n","   ├─ test\n","   ├─ train\n","└─ result\n","```"]},{"cell_type":"markdown","metadata":{"id":"n8YuhhVNSL48"},"source":["#連結Google drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kDtbJAbS201N"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sfinzBRi27yj"},"outputs":[],"source":["import os\n","os.chdir('/content/drive/My Drive/Deep_X_torch') #切換目錄\n","os.listdir() #確認目錄內容"]},{"cell_type":"markdown","metadata":{"id":"Pd2urWFtRs2f"},"source":["#將DCM檔轉成JPG檔"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DTcpC-wJSoOc"},"outputs":[],"source":["!pip install pydicom\n","!pip install opencv-python"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o3YCwVLTW7YD"},"outputs":[],"source":["from itertools import count\n","import pydicom\n","import cv2\n","import os\n","\n","def dcm_to_png(folder_path,output_path,output_name):\n","    count = 1\n","    file_count = 0\n","    elbow_count = 0\n","    for file_name in os.listdir(folder_path):\n","        if file_name.endswith(\".dcm\"):\n","            file_count += 1\n","            try:\n","                ds = pydicom.dcmread(os.path.join(folder_path, file_name))\n","                # 檢索患者年齡\n","                patient_name = ds.PatientName\n","                uid = ds.SeriesInstanceUID\n","                patient_age = ds.PatientAge\n","                patient_part = ds.BodyPartExamined\n","\n","                if patient_part == \"ELBOW\":\n","                  elbow_count += 1\n","\n","                # 打印患者年齡\n","                # print(\"Filename:\",file_name)\n","                # print(\"classified:\",output_name)\n","                # print(\"Patient's Name:\", patient_name)\n","                # print(\"UID:\", uid)\n","                # print(\"Patient's Age:\", patient_age)\n","                # print(\"Patient's body part:\", patient_part)\n","                # print(\"\\n\\n\")\n","            except pydicom.errors.InvalidDicomError:\n","                continue\n","\n","            data = ds.pixel_array\n","            image = cv2.normalize(data, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n","\n","            # calculate the rotation matrix\n","            rotation_matrix = cv2.getRotationMatrix2D((image.shape[1]/2, image.shape[0]/2), 0, 1.0)\n","\n","            # apply the rotation matrix to the image\n","            rotated = cv2.warpAffine(image, rotation_matrix, (image.shape[1], image.shape[0]))\n","\n","            # adjust the brightness of the rotated image\n","            bright_image = cv2.convertScaleAbs(rotated, alpha=1, beta=0)\n","\n","            # create a new folder for each PNG image\n","            folder_name = os.path.join(os.getcwd(), output_path, output_name + \"(.jpg)\")\n","            os.makedirs(folder_name, exist_ok=True)\n","\n","            # save the enhanced image as a PNG file inside the folder\n","            file_prefix = os.path.splitext(file_name)[0]\n","            cv2.imwrite(os.path.join(folder_name, f\"{output_name}_{count}.jpg\"), bright_image)\n","            count += 1\n","\n","    print(output_name)\n","    print(\"File numbers:\" + str(file_count))\n","    print(\"Elbow numbers: \" + str(elbow_count))\n","\n","# set the path to the folder containing the DICOM images\n","dcm_path_abnormal = '/content/drive/My Drive/Deep_X_torch/original_dataset/abnormal/abnormal(.dcm)'\n","output_path_abnormal = '/content/drive/My Drive/Deep_X_torch/original_dataset/abnormal'\n","dcm_path_normal = '/content/drive/My Drive/Deep_X_torch/original_dataset/normal/normal(.dcm)'\n","output_path_normal = '/content/drive/My Drive/Deep_X_torch/original_dataset/normal'\n","\n","dcm_to_png(dcm_path_abnormal, output_path_abnormal, 'abnormal')\n","dcm_to_png(dcm_path_normal, output_path_normal, 'normal')\n","\n","# dcm_path_test = '/content/drive/My Drive/Deep_X_torch/original_dataset/test/test(.dcm)'\n","# output_path_test = '/content/drive/My Drive/Deep_X_torch/original_dataset/test'\n","\n","# dcm_to_png(dcm_path_test, output_path_test, 'test')"]},{"cell_type":"markdown","metadata":{"id":"hsQhWruTS-HN"},"source":["#資料前處理"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mxsk1T72ib7I"},"outputs":[],"source":["import cv2\n","import os\n","\n","def remove_noise(img, kernel_size=(9, 9)):\n","    \"\"\"去除噪聲\"\"\"\n","    blurred = cv2.GaussianBlur(img, kernel_size, 0)\n","    return blurred\n","\n","def gray_scale(img):\n","    \"\"\"轉換為灰度影像\"\"\"\n","    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","    return gray\n","\n","def process_images(input_dir, output_dir):\n","    for filename in os.listdir(input_dir):\n","        if filename.endswith('.jpg'):\n","            input_path = os.path.join(input_dir, filename)\n","            if not os.path.exists(output_dir):\n","              os.makedirs(output_dir)\n","            output_path = os.path.join(output_dir, filename)\n","\n","            img = cv2.imread(input_path)\n","            clahe = cv2.createCLAHE(clipLimit=3)\n","            gray_img = gray_scale(img)\n","            clahe_img = clahe.apply(gray_img)\n","            cv2.imwrite(output_path, clahe_img)\n","\n","normal_input_dir = '/content/drive/My Drive/Deep_X_torch/original_dataset/normal/normal(.jpg)'\n","normal_output_dir = '/content/drive/My Drive/Deep_X_torch/processed_dataset/normal'\n","\n","abnormal_input_dir = '/content/drive/My Drive/Deep_X_torch/original_dataset/abnormal/abnormal(.jpg)'\n","abnormal_output_dir = '/content/drive/My Drive/Deep_X_torch/processed_dataset/abnormal'\n","\n","yolo_output_dir = '/content/drive/My Drive/Deep_X_torch/all_data'\n","\n","process_images(normal_input_dir, normal_output_dir)\n","process_images(abnormal_input_dir, abnormal_output_dir)\n","\n","process_images(normal_input_dir, yolo_output_dir)\n","process_images(abnormal_input_dir, yolo_output_dir)"]},{"cell_type":"markdown","metadata":{"id":"ZnUUgJ0iOtxE"},"source":["#區分訓練集(train)和測試集(test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yPhvl_dQOsUI"},"outputs":[],"source":["#Split the raw dataset into train set, valid set and test set.\n","import os\n","import random\n","import shutil\n","\n","TRAIN_SET_RATIO = 0.7\n","TEST_SET_RATIO = 0.3\n","\n","class SplitDataset():\n","    def __init__(self, dataset_dir, saved_dataset_dir, train_ratio=TRAIN_SET_RATIO, test_ratio=TEST_SET_RATIO, show_progress=False):\n","        self.dataset_dir = '/content/drive/My Drive/Deep_X_torch/processed_dataset/'\n","        self.saved_dataset_dir = '/content/drive/My Drive/Deep_X_torch/splitted_dataset/'\n","        self.saved_train_dir = '/content/drive/My Drive/Deep_X_torch/splitted_dataset/train/'\n","        self.saved_test_dir = '/content/drive/My Drive/Deep_X_torch/splitted_dataset/test/'\n","\n","        self.train_ratio = train_ratio\n","        self.test_radio = test_ratio\n","\n","        self.train_file_path = []\n","        self.test_file_path = []\n","\n","        self.index_label_dict = {}\n","\n","        self.show_progress = show_progress\n","\n","        if not os.path.exists(self.saved_train_dir):\n","            os.mkdir(self.saved_train_dir)\n","        if not os.path.exists(self.saved_test_dir):\n","            os.mkdir(self.saved_test_dir)\n","\n","\n","    def __get_label_names(self):\n","        label_names = []\n","        for item in os.listdir(self.dataset_dir):\n","            item_path = os.path.join(self.dataset_dir, item)\n","            if os.path.isdir(item_path):\n","                label_names.append(item)\n","        return label_names\n","\n","    def __get_all_file_path(self):\n","        all_file_path = []\n","        index = 0\n","        for file_type in self.__get_label_names():\n","            self.index_label_dict[index] = file_type\n","            index += 1\n","            type_file_path = os.path.join(self.dataset_dir, file_type)\n","            file_path = []\n","            for file in os.listdir(type_file_path):\n","                single_file_path = os.path.join(type_file_path, file)\n","                file_path.append(single_file_path)\n","            all_file_path.append(file_path)\n","        return all_file_path\n","\n","    def __copy_files(self, type_path, type_saved_dir):\n","        for item in type_path:\n","            src_path_list = item[1]\n","            dst_path = type_saved_dir + \"%s/\" % (item[0])\n","            if not os.path.exists(dst_path):\n","                os.mkdir(dst_path)\n","            for src_path in src_path_list:\n","                shutil.copy(src_path, dst_path)\n","                if self.show_progress:\n","                    print(\"Copying file \"+src_path+\" to \"+dst_path)\n","\n","    def __split_dataset(self):\n","        all_file_paths = self.__get_all_file_path()\n","        for index in range(len(all_file_paths)):\n","            file_path_list = all_file_paths[index]\n","            file_path_list_length = len(file_path_list)\n","            random.shuffle(file_path_list)\n","\n","            train_num = int(file_path_list_length * self.train_ratio)\n","            test_num = int(file_path_list_length * self.test_radio)\n","            test_num += 1\n","\n","            self.train_file_path.append([self.index_label_dict[index], file_path_list[: train_num]])\n","            self.test_file_path.append([self.index_label_dict[index], file_path_list[train_num:train_num + test_num]])\n","\n","    def start_splitting(self):\n","        self.__split_dataset()\n","        self.__copy_files(type_path=self.train_file_path, type_saved_dir=self.saved_train_dir)\n","        self.__copy_files(type_path=self.test_file_path, type_saved_dir=self.saved_test_dir)\n","\n","\n","if __name__ == '__main__':\n","    split_dataset = SplitDataset(dataset_dir='/content/drive/My Drive/Deep_X_torch/processed_dataset/',\n","                                 saved_dataset_dir='/content/drive/My Drive/Deep_X_torch/splitted_dataset/',\n","                                 show_progress=True)\n","    split_dataset.start_splitting()"]},{"cell_type":"markdown","metadata":{"id":"kgU18ivPXfIq"},"source":["#訓練集資料增強\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jgOJ_cRl9Kio"},"outputs":[],"source":["!pip install imagecorruptions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IudZMcitXoHB"},"outputs":[],"source":["import glob\n","import cv2\n","import imgaug.augmenters as iaa\n","import os\n","from tqdm import trange\n","\n","def data_augmentation(input_path, output_path, yolo_output_path, times):\n","    # Define a set of image augmentation operations using imgaug\n","    sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n","    seq = iaa.Sequential([\n","        iaa.SomeOf((0, 5), [\n","            iaa.Fliplr(0.5),  # 有 50% 的概率水平翻轉\n","            iaa.Flipud(0.5),  # 有 50% 的概率垂直翻轉\n","            iaa.Affine(rotate=(-20, 20)),  # 隨機旋轉圖像 -10 到 10 度\n","            iaa.OneOf([\n","                iaa.GaussianBlur((0, 1.5)),  # 高斯模糊，模糊程度在 0 到 1.5 之間\n","                iaa.AverageBlur(k=(2, 5)),   # 均值模糊，核的大小在 2 到 5 之間\n","                iaa.MedianBlur(k=(3, 7)),   # 中值模糊，核的大小在 3 到 8 之間\n","            ]),\n","            # iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)),  # 銳化，參數可調\n","            # iaa.Emboss(alpha=(0, 1.0), strength=(0, 0.5)),  # 浮雕效果，參數可調\n","            # iaa.Add((-5, 5), per_channel=0.5),  # 添加亮度，每通道亮度值在 -10 到 10 之間\n","            # iaa.Multiply((0.5, 1.5)),  # 乘以亮度因子，值在 0.5 到 1.5 之間\n","            iaa.contrast.LinearContrast((0.7, 1.2)),  # 線性對比度，參數可調\n","            iaa.imgcorruptlike.Saturate(severity=3),  # 飽和度增強，程度為 3\n","        ], random_order=True)  # 隨機應用上述操作，順序隨機\n","    ], random_order=True)\n","\n","    # Create the output directory if it doesn't exist\n","    if not os.path.exists(output_path):\n","        os.makedirs(output_path)\n","\n","    if not os.path.exists(yolo_output_path):\n","        os.makedirs(yolo_output_path)\n","\n","    # Process each file in the input_path directory\n","    # Process each .jpg file in the input_path directory\n","    file_count = 0\n","    for jpg_file in glob.glob(os.path.join(input_path, '*.jpg')):\n","        img = cv2.imread(jpg_file)\n","        img_list = [img]  # Create a list with a single image\n","\n","        for count in trange(times):\n","            images_aug = seq.augment_images(img_list)\n","            for index, augmented_image in enumerate(images_aug):\n","                filename = os.path.splitext(os.path.basename(jpg_file))[0]  # Extract the filename without extension\n","                output1 = os.path.join(output_path, f\"{filename}_aug{count + 1}.jpg\")\n","                output2 = os.path.join(yolo_output_path, f\"{filename}_aug{count + 1}.jpg\")\n","                cv2.imwrite(output1, augmented_image)\n","                cv2.imwrite(output2, augmented_image)\n","        file_count += 1\n","\n","\n","    # Calculate and print statistics\n","    print(\"增強前圖片數量：\" + str(file_count))\n","    print(\"增強後圖片數量：\" + str(file_count * times))\n","    print(\"數據增強完成\")\n","\n","# Example usage:\n","yolo_output_dir = '/content/drive/My Drive/Deep_X_torch/all_data'\n","normal_input_dir = '/content/drive/My Drive/Deep_X_torch/splitted_dataset/train/normal'\n","normal_output_dir = '/content/drive/My Drive/Deep_X_torch/splitted_dataset/train_argumentation/normal'\n","abnormal_input_dir = '/content/drive/My Drive/Deep_X_torch/splitted_dataset/train/abnormal'\n","abnormal_output_dir = '/content/drive/My Drive/Deep_X_torch/splitted_dataset/train_argumentation/abnormal'\n","\n","data_augmentation(normal_input_dir, normal_output_dir, yolo_output_dir, 20)\n","data_augmentation(abnormal_input_dir, abnormal_output_dir, yolo_output_dir, 20)\n","\n","# test_dir = '/content/drive/My Drive/Deep_X_torch/original_dataset/test/test(.jpg)'\n","\n","# data_augmentation(test_dir, test_dir, 20)\n"]},{"cell_type":"markdown","metadata":{"id":"5kK8KIsoP0Kf"},"source":["#YoloV8自動化標記"]},{"cell_type":"markdown","metadata":{"id":"KyQhWZ2LfEc2"},"source":["##YoloV8資料結構\n","```\n","└── YOLOv8\n","    └── yolov8m\n","         └── args.yaml\n","└── classes.txt\n","└── data_custom.yaml\n","└── result\n","    └── source\n","└── runs\n","    └── detect\n","└── train\n","    └── images\n","    └── labels\n","└── val\n","    └── images\n","    └── labels\n","└── yolov8m.pt\n","```"]},{"cell_type":"markdown","metadata":{"id":"-qw_5dxUfMIJ"},"source":["##下載函式庫"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u4VK_9wKe3MK"},"outputs":[],"source":["#!pip install simple_image_download"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tl-lXAkv8l2g"},"outputs":[],"source":["!pip install ultralytics\n","from ultralytics import YOLO\n","import ultralytics\n","ultralytics.checks()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u6PLmr27HCcM"},"outputs":[],"source":["import os\n","os.chdir('/content/drive/My Drive/Deep_X_torch/yolov8_custom') #切換目錄\n","os.listdir() #確認目錄內容"]},{"cell_type":"markdown","metadata":{"id":"ZhJ0h0k8lRtk"},"source":["##訓練(train)Yolo模型"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wSkRTMralRtr"},"outputs":[],"source":["!yolo task=detect mode=train epochs=350 data=data_custom.yaml model=yolov8m.pt imgsz=640 batch=15 patience=0"]},{"cell_type":"markdown","metadata":{"id":"E8FtE3kptiEo"},"source":["##驗證(predict)Yolo模型"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1pBh5qMilRtr"},"outputs":[],"source":["!yolo task=detect mode=predict save=True model=/content/drive/MyDrive/Deep_X_torch/yolov8_custom/runs/detect/train2/weights/best.pt conf=0.3 source=/content/drive/MyDrive/Deep_X_torch/all_data save_txt=True save_crop=False max_det=1"]},{"cell_type":"markdown","metadata":{"id":"UGOZzdmVR02k"},"source":["#根據標記的label切割JPG檔案"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zFkTIcTqujBh"},"outputs":[],"source":["import cv2\n","import os\n","\n","def crop_images(img_folder, label_folder, save_folder, crop_size=(224, 224)):\n","    # create new folder to save the cropped images\n","    os.makedirs(save_folder, exist_ok=True)\n","\n","    # loop through each image file in the folder\n","    for img_file in os.listdir(img_folder):\n","        # check if the file is an image file\n","        if not img_file.endswith('.jpg'):\n","            continue\n","\n","        # read the corresponding text file\n","        txt_file = img_file.split(\".\")[0]\n","        txt_file = txt_file + '.txt'\n","        # txt_file = txt_file.split(\"_\")[0] + \"_\" + txt_file.split(\"_\")[1] + '.txt'\n","        txt_path = os.path.join(label_folder, txt_file)\n","        print(txt_file)\n","        if not os.path.exists(txt_path):\n","            continue\n","\n","        # read the image\n","        img_path = os.path.join(img_folder, img_file)\n","        img = cv2.imread(img_path)\n","\n","        # read the bounding box and class label from the text file\n","        with open(txt_path, 'r') as f:\n","            line = f.readline()\n","            class_id, x_center, y_center, width, height = [float(x) for x in line.split()]\n","\n","        # Convert coordinates to the top-left and bottom-right corners of the image\n","        x_min = int((x_center - width / 2) * img.shape[1])\n","        y_min = int((y_center - height / 2) * img.shape[0])\n","        x_max = int((x_center + width / 2) * img.shape[1])\n","        y_max = int((y_center + height / 2) * img.shape[0])\n","\n","        # Crop the image and save it to the save folder\n","        cropped_img = img[y_min:y_max, x_min:x_max]\n","        cropped_img = cv2.resize(cropped_img, crop_size)\n","        save_path = os.path.join(save_folder, img_file)\n","        cv2.imwrite(save_path, cropped_img)\n","\n","normal_label_folder = '/content/drive/My Drive/Deep_X_torch/original_dataset/normal/normal_label'\n","abnormal_label_folder = '/content/drive/My Drive/Deep_X_torch/original_dataset/abnormal/abnormal_label'\n","\n","yolo_label_folder = '/content/drive/MyDrive/Deep_X_torch/yolov8_custom/runs/detect/predict2/labels'\n","\n","train_normal_dir = '/content/drive/My Drive/Deep_X_torch/splitted_dataset/train/normal'\n","train_abnormal_dir = '/content/drive/My Drive/Deep_X_torch/splitted_dataset/train/abnormal'\n","train_argumentation_normal_dir = '/content/drive/My Drive/Deep_X_torch/splitted_dataset/train_argumentation/normal'\n","train_argumentation_abnormal_dir = '/content/drive/My Drive/Deep_X_torch/splitted_dataset/train_argumentation/abnormal'\n","test_normal_dir = '/content/drive/My Drive/Deep_X_torch/splitted_dataset/test/normal'\n","test_abnormal_dir = '/content/drive/My Drive/Deep_X_torch/splitted_dataset/test/abnormal'\n","\n","# crop_images(train_normal_dir, normal_label_folder, train_normal_dir)\n","# crop_images(train_abnormal_dir, abnormal_label_folder, train_abnormal_dir)\n","# crop_images(train_argumentation_normal_dir, normal_label_folder, train_argumentation_normal_dir)\n","# crop_images(train_argumentation_abnormal_dir, abnormal_label_folder, train_argumentation_abnormal_dir)\n","# crop_images(test_normal_dir, normal_label_folder, test_normal_dir)\n","# crop_images(test_abnormal_dir, abnormal_label_folder, test_abnormal_dir)\n","\n","crop_images(train_normal_dir, yolo_label_folder, train_normal_dir)\n","crop_images(train_abnormal_dir, yolo_label_folder, train_abnormal_dir)\n","crop_images(train_argumentation_normal_dir, yolo_label_folder, train_argumentation_normal_dir)\n","crop_images(train_argumentation_abnormal_dir, yolo_label_folder, train_argumentation_abnormal_dir)\n","crop_images(test_normal_dir, yolo_label_folder, test_normal_dir)\n","crop_images(test_abnormal_dir, yolo_label_folder, test_abnormal_dir)"]},{"cell_type":"markdown","metadata":{"id":"2Z0QXaeMnBIu"},"source":["#將圖像轉換為張量"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QbIXFzMJPVZy"},"outputs":[],"source":["import os\n","from PIL import Image\n","import torch\n","from torchvision import transforms\n","\n","def transform(image_path,tensor_path):\n","    # 設置轉換方式，將圖像轉換為張量\n","    transform = transforms.Compose([\n","        transforms.ToTensor()\n","    ])\n","\n","    # 遍歷資料夾中的所有檔案\n","    for filename in os.listdir(image_path):\n","        # 讀取圖像\n","        img_path = os.path.join(image_path, filename)\n","        img = Image.open(img_path)\n","\n","        file_name,extension = os.path.splitext(filename)\n","\n","        # 將圖像轉換為張量\n","        tensor_img = transform(img)\n","\n","        # 將張量保存為.pt檔案\n","        tensors_path = os.path.join(tensor_path, file_name + \".pt\")\n","        #print(tensors_path)\n","        torch.save(tensor_img,tensors_path)\n","\n","train_normal_splitted_path = '/content/drive/My Drive/Deep_X_torch/splitted_dataset/train_argumentation/normal'\n","train_abnormal_splitted_path = '/content/drive/My Drive/Deep_X_torch/splitted_dataset/train_argumentation/abnormal'\n","train_normal_tensor_path = '/content/drive/My Drive/Deep_X_torch/tensor/train/normal'\n","train_abnormal_tensor_path = '/content/drive/My Drive/Deep_X_torch/tensor/train/abnormal'\n","test_normal_splitted_path = '/content/drive/My Drive/Deep_X_torch/splitted_dataset/test/normal'\n","test_abnormal_splitted_path = '/content/drive/My Drive/Deep_X_torch/splitted_dataset/test/abnormal'\n","test_normal_tensor_path = '/content/drive/My Drive/Deep_X_torch/tensor/test/normal'\n","test_abnormal_tensor_path = '/content/drive/My Drive/Deep_X_torch/tensor/test/abnormal'\n","\n","transform(train_normal_splitted_path,train_normal_tensor_path)\n","transform(train_abnormal_splitted_path,train_abnormal_tensor_path)\n","transform(test_normal_splitted_path,test_normal_tensor_path)\n","transform(test_abnormal_splitted_path,test_abnormal_tensor_path)"]},{"cell_type":"markdown","metadata":{"id":"mtkv4P6wKNkv"},"source":["#訓練及驗證模型(ResNet/5-Fold)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zVqVkuZTZmb-"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import os\n","import numpy as np\n","import torch.optim as optim\n","import torchvision.models as models\n","import matplotlib.pyplot as plt\n","import csv\n","import random\n","from torch.utils.data import DataLoader, Dataset\n","from torch.utils.data import ConcatDataset\n","from sklearn import datasets\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import confusion_matrix\n","from tqdm import tqdm\n","from torch.utils.tensorboard import SummaryWriter\n","from datetime import datetime\n","\n","class MyDataset(Dataset):\n","    def __init__(self, data_path):\n","        self.data_path = data_path\n","        self.class_to_idx = {'abnormal': 1, 'normal': 0}  # 定義類別名稱到類別索引的映射\n","        self.data = []\n","        self.filenames = []  # store filenames\n","        for filename in os.listdir(data_path):\n","            if filename.endswith('.pt'):\n","                tensor = torch.load(os.path.join(data_path, filename))\n","                if filename.split('_')[0] == 'normal':\n","                    label_idx = 0\n","                else:\n","                    label_idx = 1\n","                self.data.append((tensor, label_idx, filename))\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        tensor, labels, filename = self.data[index]\n","        return tensor, labels, filename\n","\n","class CombinedDataset(ConcatDataset):\n","    def __init__(self, dataset1, dataset2):\n","        super().__init__([dataset1, dataset2])\n","\n","    def __getitem__(self, index):\n","        return super().__getitem__(index)\n","\n","    def __len__(self):\n","        return super().__len__()\n","\n","\n","# 建立資料夾顯示訓練結果\n","def mkdir_outcome(result_path):\n","    file_names = os.listdir(result_path)\n","    num_max = 0\n","    for file_name in file_names:\n","        if file_name.startswith(\"result_\"):\n","            num_str = file_name.split(\"_\")[1]\n","            num = int(num_str)\n","            if(num > num_max):\n","                num_max = num\n","    # make folder for train result\n","    result_path = os.path.join(result_path,\"result_{}\".format(num_max + 1))\n","    result_path_train = os.path.join(result_path,\"train_{}\".format(num_max + 1))\n","    os.makedirs(result_path,exist_ok=True)\n","    os.makedirs(result_path_train,exist_ok=True)\n","    return result_path_train\n","\n","\n","# 模型評估指標\n","def validation_index(conf_matrix):\n","    # Confusion Matrix to calculate [accuracy,precision,recall]\n","    precision = 0.0\n","    recall = 0.0\n","    f1_score = 0.0\n","    if((conf_matrix[0][0] + conf_matrix[0][1]) != 0):\n","        precision = conf_matrix[0][0] / (conf_matrix[0][0] + conf_matrix[0][1])\n","    if((conf_matrix[0][0] + conf_matrix[1][0]) != 0):\n","        recall = conf_matrix[0][0] / (conf_matrix[0][0] + conf_matrix[1][0])\n","    if((precision + recall) != 0):\n","        f1_score = 2*precision*recall / (precision + recall)\n","    TPR = recall\n","    FPR = conf_matrix[0][1] / (conf_matrix[0][1] + conf_matrix[1][1])\n","    print(\"\\t      Precision: {:<.4f}  -  Recall: {:<.4f}  -  F1 Score: {:<.4f}\".format(precision,recall,f1_score))\n","    return precision,recall,f1_score,TPR,FPR\n","\n","\n","# 混淆矩陣\n","def Confusion_Matrix(result_path,conf_matrix,fold_nums):\n","    # Create the 'confusion_matrix_record' directory if it doesn't exist\n","    confusion_matrix_record_dir = os.path.join(result_path, 'confusion_matrix_record')\n","    plt.clf()\n","    if not os.path.exists(confusion_matrix_record_dir):\n","        os.makedirs(confusion_matrix_record_dir)\n","\n","    confusion_matrix = np.array([[conf_matrix[0][0], conf_matrix[0][1]], [conf_matrix[1][0], conf_matrix[1][1]]])\n","    print(\"Confusion matrix:\")\n","    print(conf_matrix)\n","    plt.imshow(confusion_matrix, cmap=plt.cm.Blues, interpolation='nearest')\n","    plt.colorbar()\n","\n","    # confusion matrix index 各個 index 的數值\n","    for i in range(2):\n","        for j in range(2):\n","            text_color = 'black' if confusion_matrix[i][j] < 0.5 * confusion_matrix.max() else 'white'\n","            plt.annotate(str(confusion_matrix[i][j]), xy=(j, i), ha='center', va='center', color=text_color)\n","    tick_marks = np.arange(2)\n","    plt.xticks(tick_marks, ['Positive', 'Negative'])\n","    plt.yticks(tick_marks, ['Positive', 'Negative'])\n","    plt.ylabel('Predicted Label')\n","    plt.xlabel('True Label')\n","    plt.title('Confusion Matrix')\n","    result_path = result_path + '/confusion_matrix_record'\n","    plt.savefig(os.path.join(result_path,'confusion_matrix_fold'+str(fold_nums + 1)+'.png'))\n","\n","\n","# ROC曲線\n","def ROC_Curve(result_path,tpr_list,fpr_list,fold_nums):\n","    # Create the 'roc_curve_record' directory if it doesn't exist\n","    roc_curve_record_dir = os.path.join(result_path, 'roc_curve_record')\n","    if not os.path.exists(roc_curve_record_dir):\n","        os.makedirs(roc_curve_record_dir)\n","\n","    # 計算 AUC\n","    roc_auc = np.trapz(tpr_list, fpr_list)\n","\n","    # 繪製 ROC 曲線\n","    plt.clf()\n","    plt.plot(fpr_list, tpr_list, lw=1, label='ROC (AUC = %0.2f)' % roc_auc)\n","    plt.plot([0, 1], [0, 1], '--', color='gray', label='Random Guessing')\n","    plt.xlim([-0.05, 1.05])\n","    plt.ylim([-0.05, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Receiver Operating Characteristic (ROC) Curve')\n","    plt.legend()\n","    result_path = result_path + '/roc_curve_record'\n","    plt.savefig(os.path.join(result_path,'Roc_curve'+str(fold_nums + 1)+'.png'))\n","\n","\n","# 輸出每一次 epoch 的結果\n","def CSV_Output(result_path,parameter,num_epochs,train_loss_list,train_acc_list,val_loss_list,val_acc_list,precision_list,recall_list,TPR_list,FPR_list,f1_score_list,fold_wrong_predict,fold_nums):\n","    # Create the 'csv_record' directory if it doesn't exist\n","    csv_record_dir = os.path.join(result_path, 'csv_record')\n","    if not os.path.exists(csv_record_dir):\n","        os.makedirs(csv_record_dir)\n","    with open(result_path + '/csv_record/epoch_fold'+str(fold_nums + 1)+'.csv','w',newline='') as file:\n","        writer = csv.writer(file)\n","        writer.writerow(['num_epochs','batch_size','learning_rate','num_classes','device','start_time','end_time','num_fold'])\n","        writer.writerow(parameter)\n","        writer.writerow('')\n","        writer.writerow(['Epoch','train_loss','train_acc','val_loss','val_acc','precision','recall','TPR','FPR','F1 score'])\n","        for epoch in range(num_epochs):\n","            writer.writerow([epoch + 1,\n","                            round(train_loss_list[epoch], 4),\n","                            round(train_acc_list[epoch].item(), 4),\n","                            round(val_loss_list[epoch], 4),\n","                            round(val_acc_list[epoch].item(), 4),\n","                            round(precision_list[epoch], 4),\n","                            round(recall_list[epoch], 4),\n","                            round(TPR_list[epoch], 4),\n","                            round(FPR_list[epoch], 4),\n","                            round(f1_score_list[epoch], 4)])\n","        writer.writerow([f'wrong_predict:',fold_wrong_predict])\n","    print('CSV output Sucessfully')\n","\n","def print_filename_in_txt(train_loader, valid_loader, result_path, fold_num):\n","    # Initialize empty lists to store filenames\n","    train_filenames = []\n","    valid_filenames = []\n","\n","    # Iterate over the training dataset\n","    for batch_idx, (data, target, filename) in enumerate(train_loader):\n","        # Append the filename to the list\n","        train_filenames.extend(filename)\n","\n","    # Iterate over the validation dataset\n","    for batch_idx, (data, target, filename) in enumerate(valid_loader):\n","        # Append the filename to the list\n","        valid_filenames.extend(filename)\n","\n","    # Create the 'file_name_record' directory if it doesn't exist\n","    file_record_dir = os.path.join(result_path, 'file_name_record')\n","    if not os.path.exists(file_record_dir):\n","        os.makedirs(file_record_dir)\n","\n","    # Save the training filenames as a text file\n","    with open(result_path + '/file_name_record/train_filenames_fold_'+str(fold_num + 1)+'.txt', 'w') as file:\n","        for filename in train_filenames:\n","            file.write(filename + '\\n')\n","\n","    # Save the validation filenames as a text file\n","    with open(result_path + '/file_name_record/valid_filenames_fold_'+str(fold_num + 1)+'.txt', 'w') as file:\n","        for filename in valid_filenames:\n","            file.write(filename + '\\n')\n","\n","    print(f\"Filenames saved in {result_path}/train_filenames.txt and {result_path}/valid_filenames.txt\")\n","\n","def calculate_average(avg_train_acc_list, avg_val_acc_list, avg_recall_list, avg_precision_list, avg_f1_score_list, avg_TPR_list, avg_FPR_list, fold_nums, result_path):\n","    # Calculate averages\n","    avg_train_acc = sum(avg_train_acc_list) / fold_nums\n","    avg_val_acc = sum(avg_val_acc_list) / fold_nums\n","    avg_recall = sum(avg_recall_list) / fold_nums\n","    avg_precision = sum(avg_precision_list) / fold_nums\n","    avg_f1_score = sum(avg_f1_score_list) / fold_nums\n","    avg_TPR = sum(avg_TPR_list) / fold_nums\n","    avg_FPR = sum(avg_FPR_list) / fold_nums\n","\n","    # Save averages to a text file\n","    with open(result_path + '/Average.txt', 'w') as file:\n","        file.write(f\"Avg Train Accuracy: {avg_train_acc}\\n\")\n","        file.write(f\"Avg Validation Accuracy: {avg_val_acc}\\n\")\n","        file.write(f\"Avg Recall: {avg_recall}\\n\")\n","        file.write(f\"Avg Precision: {avg_precision}\\n\")\n","        file.write(f\"Avg F1 Score: {avg_f1_score}\\n\")\n","        file.write(f\"Avg TPR: {avg_TPR}\\n\")\n","        file.write(f\"Avg FPR: {avg_FPR}\\n\")\n","\n","    print(f\"Averages saved to {result_path}\")\n","\n","def train(normal_data_dir,abnormal_data_dir,train_normal_tensor_path,train_abnormal_tensor_path,result_path,num_fold):\n","    #超參數設定\n","    batch_size = 52\n","    learning_rate = 0.001\n","    num_epochs = 50\n","    num_classes = 2\n","    num_folds = num_fold\n","    start_time = datetime.now()\n","    end_time = 0\n","    num_argumentation = 20\n","    conf_matrix = np.zeros((num_classes, num_classes), dtype=np.int32)\n","\n","    # Get the dataset\n","    train_normal_dataset = MyDataset(train_normal_tensor_path)\n","    train_abnormal_dataset = MyDataset(train_abnormal_tensor_path)\n","    # Combine the datasets\n","    train_datasets = CombinedDataset(train_normal_dataset,train_abnormal_dataset)\n","\n","    # Create the k-fold cross-validation object\n","    kfold = KFold(n_splits=num_folds)\n","\n","    #印出資料集大小\n","    print(\"train dataset's size : \" + str(len(train_datasets)))\n","\n","    #創建模型\n","    model = models.resnet152(pretrained=True)\n","    model.fc = nn.Linear(model.fc.in_features, num_classes)\n","    #print(model)\n","    #print(model.fc)\n","\n","    #將模型移動到GPU上進行運算\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model.to(device)\n","    model.fc.to(device)\n","    print(\"Device used : \" + str(device))\n","\n","    # make dir to save the training outcome\n","    result_path = mkdir_outcome(result_path)\n","\n","    avg_train_acc_list = []\n","    avg_val_acc_list = []\n","    avg_recall_list = []\n","    avg_precision_list = []\n","    avg_f1_score_list = []\n","    avg_TPR_list = []\n","    avg_FPR_list = []\n","\n","    files = []\n","    file_names = os.listdir(normal_data_dir)\n","    for file_name in file_names:\n","        name = os.path.splitext(file_name)[0]\n","        files.append(name)\n","\n","    file_names = os.listdir(abnormal_data_dir)\n","    for file_name in file_names:\n","        name = os.path.splitext(file_name)[0]\n","        files.append(name)\n","\n","    random.shuffle(files)\n","\n","    torch.save(model.state_dict(),os.path.join(result_path,\"resnet152_pre_train.pt\"))\n","\n","    for fold, (train_indices, valid_indices) in enumerate(kfold.split(files)):\n","        print(f\"Fold: {fold+1}\")\n","\n","        #定義損失函數和優化器\n","        m = nn.Sigmoid()\n","        criterion = nn.BCELoss()\n","        # criterion = nn.CrossEntropyLoss()\n","        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n","\n","        train_loss_list = []\n","        train_acc_list = []\n","        val_loss_list = []\n","        val_acc_list = []\n","        recall_list = []\n","        precision_list = []\n","        f1_score_list = []\n","        TPR_list = []\n","        FPR_list = []\n","        fold_wrong_predict = []\n","\n","        train_files = [files[i] for i in train_indices]\n","        valid_files = [files[i] for i in valid_indices]\n","\n","        # 定義用於儲存訓練集和驗證集的檔案名稱的列表\n","        train_data_files = []\n","        valid_data_files = []\n","\n","        # 尋找所有訓練集資料檔名，根據增強資料的命名規則，找到對應的增強檔名\n","        for file in train_files:\n","            labels, index = file.split('_')\n","            # 建構增強後資料的檔名\n","            for num in range(1,num_argumentation + 1):\n","              augmented_file = f\"{labels}_{index}_aug{num}.pt\"\n","              # 將增強後資料的檔案名稱新增至訓練集檔案名稱列表\n","              train_data_files.append(augmented_file)\n","\n","        # 尋找所有驗證集資料檔名，根據增強資料的命名規則，找到對應的增強檔名\n","        for file in valid_files:\n","            labels, index = file.split('_')\n","            # 建構增強後資料的檔名\n","            for num in range(1,num_argumentation + 1):\n","              augmented_file = f\"{labels}_{index}_aug{num}.pt\"\n","              valid_data_files.append(augmented_file)\n","\n","        train = []\n","        valid = []\n","\n","        # 尋找所有combined_dataset中的樣本索引\n","        for index, (data, labels, filename) in enumerate(train_datasets):\n","            # 檢查目前樣本的檔案名稱是否在訓練集檔案名稱清單中\n","            if filename in train_data_files:\n","                train.append(index)\n","            # 檢查目前樣本的檔案名稱是否在驗證集檔案名稱清單中\n","            elif filename in valid_data_files:\n","                valid.append(index)\n","\n","        # Create the train and validation datasets for this fold\n","        train_dataset = torch.utils.data.Subset(train_datasets, train)\n","        valid_dataset = torch.utils.data.Subset(train_datasets, valid)\n","\n","\n","        print(\"train_dataset : \" + str(len(train_dataset)),\",valid_dataset : \" + str(len(valid_dataset)))\n","\n","        # Create the data loaders for this fold\n","        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n","        valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=True, num_workers=4)\n","        print_filename_in_txt(train_loader,valid_loader,result_path,fold)\n","\n","        #訓練模型\n","        for epoch in range(num_epochs):\n","\n","            train_loss = 0\n","            train_correct = 0\n","            train_acc = 0\n","            val_loss = 0\n","            val_corrects = 0\n","            val_acc = 0\n","\n","            print(\"result_path:\" + str(result_path))\n","\n","            if(epoch == 0) :\n","              model.load_state_dict(torch.load(os.path.join(result_path,\"resnet152_pre_train.pt\")))\n","              print(\"Model initialized\")\n","            else :\n","              model.load_state_dict(torch.load(os.path.join(result_path,\"train_fold_\"+f'{fold+1}'+\".pt\")))\n","              print(\"Model loaded\")\n","\n","\n","            #初始化\n","            conf_matrix = np.zeros((num_classes, num_classes), dtype=np.int32)\n","\n","            print(\"[Training Progress]: \")\n","            model.train()\n","            for inputs, labels, filename in tqdm(train_loader):\n","                targets=torch.eye(2)[labels.long(), :]\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","                targets = targets.to(device)\n","\n","                optimizer.zero_grad()\n","                outputs = model(inputs)\n","                loss = criterion(m(outputs),targets.float())\n","                _, preds = torch.max(outputs, 1)\n","                train_correct += torch.sum(preds == labels.data)\n","                loss.backward()\n","                optimizer.step()\n","                train_loss += loss.item() * inputs.size(0)\n","            train_loss = train_loss / len(train_loader.dataset)\n","            train_acc = train_correct.double() / len(train_loader.dataset)\n","            train_loss_list.append(train_loss)\n","\n","            model.eval()\n","\n","            print(\"[Validating Progress]: \")\n","            wrong_predict = []\n","            for inputs, labels, filename in tqdm(valid_loader):\n","                targets=torch.eye(2)[labels.long(), :]\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","                targets = targets.to(device)\n","                with torch.set_grad_enabled(False):\n","                    outputs = model(inputs)\n","                    loss = criterion(m(outputs),targets.float())\n","                val_loss += loss.item() * inputs.size(0)\n","                _, preds = torch.max(outputs, 1)\n","                val_corrects += torch.sum(preds == labels.data)\n","\n","                # Count Confusion Matrix\n","                for t, p in zip(preds.view(-1), labels.view(-1)):\n","                    conf_matrix[t.long(), p.long()] += 1\n","                    if t != p:\n","                      wrong_predict.append(filename)\n","            val_loss = val_loss / len(valid_loader.dataset)\n","            val_acc = val_corrects.double() / len(valid_loader.dataset)\n","            val_loss_list.append(val_loss)\n","\n","            scheduler.step()\n","            end_time = datetime.now()\n","            print('\\nEpoch: [{}/{}]  train_loss: {:<.4f}  -  train_accuracy: {:<.4f} -  val_loss: {:<.4f}  -  val_accuracy: {:<.4f}  -  val_correct: {:<10}'.format(\n","                epoch+1, num_epochs, train_loss, train_acc, val_loss, val_acc, val_corrects))\n","            print('wrong predict : ' + str(wrong_predict))\n","            torch.save(model.state_dict(),os.path.join(result_path,\"train_fold_\"+f'{fold+1}'+\".pt\"))\n","            print(\"Model saved\")\n","\n","            # validation index (評估指標)\n","            precision,recall,f1_score,TPR,FPR = validation_index(conf_matrix)\n","\n","            # record the outcomes\n","            val_acc_list.append(val_acc),train_acc_list.append(train_acc),precision_list.append(precision),recall_list.append(recall)\n","            f1_score_list.append(f1_score),TPR_list.append(TPR),FPR_list.append(FPR)\n","\n","            if epoch + 1 == num_epochs:\n","              avg_train_acc_list.append(train_acc),avg_val_acc_list.append(val_acc),avg_recall_list.append(recall),avg_precision_list.append(precision),avg_f1_score_list.append(f1_score),avg_TPR_list.append(TPR),avg_FPR_list.append(FPR),fold_wrong_predict.append(wrong_predict)\n","\n","        # function of confusion matrix param(folder path, matrix, test normal dataset length, test unnormal dataset length)\n","        Confusion_Matrix(result_path,conf_matrix,fold)\n","        # functioN to show ROC curve\n","        ROC_Curve(result_path,TPR_list,FPR_list,fold)\n","        # CSV visualization\n","        param = [num_epochs,batch_size,learning_rate,num_classes,device,start_time,end_time,fold]\n","        CSV_Output(result_path,param,num_epochs,train_loss_list,train_acc_list,val_loss_list,val_acc_list,precision_list,recall_list,TPR_list,FPR_list,f1_score_list,fold_wrong_predict,fold)\n","\n","    #calculate the average\n","    calculate_average(avg_train_acc_list, avg_val_acc_list, avg_recall_list, avg_precision_list, avg_f1_score_list, avg_TPR_list, avg_FPR_list, num_folds, result_path)\n","\n","    torch.cuda.empty_cache()\n","\n","\n","normal_data_dir = '/content/drive/My Drive/Deep_X_torch/splitted_dataset/train/normal'\n","abnormal_data_dir = '/content/drive/My Drive/Deep_X_torch/splitted_dataset/train/abnormal'\n","train_normal_tensor_path = '/content/drive/My Drive/Deep_X_torch/tensor/train/normal'\n","train_abnormal_tensor_path = '/content/drive/My Drive/Deep_X_torch/tensor/train/abnormal'\n","result_path = '/content/drive/My Drive/Deep_X_torch/result_resnet152'\n","\n","train(normal_data_dir,abnormal_data_dir,train_normal_tensor_path,train_abnormal_tensor_path,result_path,5)"]},{"cell_type":"markdown","metadata":{"id":"PJjWpZj2gU-b"},"source":["#訓練及驗證模型(SEResNet/5-Fold)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ExBrYP7Wggzt"},"outputs":[],"source":["!pip install timm"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"jI3WItDbf3rO"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import os\n","import numpy as np\n","import torch.optim as optim\n","import torchvision.models as models\n","import matplotlib.pyplot as plt\n","import csv\n","import random\n","import timm\n","from torch.utils.data import DataLoader, Dataset\n","from torch.utils.data import ConcatDataset\n","from sklearn import datasets\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import confusion_matrix\n","from tqdm import tqdm\n","from torch.utils.tensorboard import SummaryWriter\n","from datetime import datetime\n","\n","class MyDataset(Dataset):\n","    def __init__(self, data_path):\n","        self.data_path = data_path\n","        self.class_to_idx = {'abnormal': 1, 'normal': 0}  # 定義類別名稱到類別索引的映射\n","        self.data = []\n","        self.filenames = []  # store filenames\n","        for filename in os.listdir(data_path):\n","            if filename.endswith('.pt'):\n","                tensor = torch.load(os.path.join(data_path, filename))\n","                if filename.split('_')[0] == 'normal':\n","                    label_idx = 0\n","                else:\n","                    label_idx = 1\n","                self.data.append((tensor, label_idx, filename))\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        tensor, labels, filename = self.data[index]\n","        return tensor, labels, filename\n","\n","class CombinedDataset(ConcatDataset):\n","    def __init__(self, dataset1, dataset2):\n","        super().__init__([dataset1, dataset2])\n","\n","    def __getitem__(self, index):\n","        return super().__getitem__(index)\n","\n","    def __len__(self):\n","        return super().__len__()\n","\n","\n","# 建立資料夾顯示訓練結果\n","def mkdir_outcome(result_path):\n","    file_names = os.listdir(result_path)\n","    num_max = 0\n","    for file_name in file_names:\n","        if file_name.startswith(\"result_\"):\n","            num_str = file_name.split(\"_\")[1]\n","            num = int(num_str)\n","            if(num > num_max):\n","                num_max = num\n","    # make folder for train result\n","    result_path = os.path.join(result_path,\"result_{}\".format(num_max + 1))\n","    result_path_train = os.path.join(result_path,\"train_{}\".format(num_max + 1))\n","    os.makedirs(result_path,exist_ok=True)\n","    os.makedirs(result_path_train,exist_ok=True)\n","    return result_path_train\n","\n","\n","# 模型評估指標\n","def validation_index(conf_matrix):\n","    # Confusion Matrix to calculate [accuracy,precision,recall]\n","    precision = 0.0\n","    recall = 0.0\n","    f1_score = 0.0\n","    if((conf_matrix[0][0] + conf_matrix[0][1]) != 0):\n","        precision = conf_matrix[0][0] / (conf_matrix[0][0] + conf_matrix[0][1])\n","    if((conf_matrix[0][0] + conf_matrix[1][0]) != 0):\n","        recall = conf_matrix[0][0] / (conf_matrix[0][0] + conf_matrix[1][0])\n","    if((precision + recall) != 0):\n","        f1_score = 2*precision*recall / (precision + recall)\n","    TPR = recall\n","    FPR = conf_matrix[0][1] / (conf_matrix[0][1] + conf_matrix[1][1])\n","    print(\"\\t      Precision: {:<.4f}  -  Recall: {:<.4f}  -  F1 Score: {:<.4f}\".format(precision,recall,f1_score))\n","    return precision,recall,f1_score,TPR,FPR\n","\n","\n","# 混淆矩陣\n","def Confusion_Matrix(result_path,conf_matrix,fold_nums):\n","    # Create the 'confusion_matrix_record' directory if it doesn't exist\n","    confusion_matrix_record_dir = os.path.join(result_path, 'confusion_matrix_record')\n","    plt.clf()\n","    if not os.path.exists(confusion_matrix_record_dir):\n","        os.makedirs(confusion_matrix_record_dir)\n","\n","    confusion_matrix = np.array([[conf_matrix[0][0], conf_matrix[0][1]], [conf_matrix[1][0], conf_matrix[1][1]]])\n","    print(\"Confusion matrix:\")\n","    print(conf_matrix)\n","    plt.imshow(confusion_matrix, cmap=plt.cm.Blues, interpolation='nearest')\n","    plt.colorbar()\n","\n","    # confusion matrix index 各個 index 的數值\n","    for i in range(2):\n","        for j in range(2):\n","            text_color = 'black' if confusion_matrix[i][j] < 0.5 * confusion_matrix.max() else 'white'\n","            plt.annotate(str(confusion_matrix[i][j]), xy=(j, i), ha='center', va='center', color=text_color)\n","    tick_marks = np.arange(2)\n","    plt.xticks(tick_marks, ['Positive', 'Negative'])\n","    plt.yticks(tick_marks, ['Positive', 'Negative'])\n","    plt.ylabel('Predicted Label')\n","    plt.xlabel('True Label')\n","    plt.title('Confusion Matrix')\n","    result_path = result_path + '/confusion_matrix_record'\n","    plt.savefig(os.path.join(result_path,'confusion_matrix_fold'+str(fold_nums + 1)+'.png'))\n","\n","\n","# ROC曲線\n","def ROC_Curve(result_path,tpr_list,fpr_list,fold_nums):\n","    # Create the 'roc_curve_record' directory if it doesn't exist\n","    roc_curve_record_dir = os.path.join(result_path, 'roc_curve_record')\n","    if not os.path.exists(roc_curve_record_dir):\n","        os.makedirs(roc_curve_record_dir)\n","\n","    # 計算 AUC\n","    roc_auc = np.trapz(tpr_list, fpr_list)\n","\n","    # 繪製 ROC 曲線\n","    plt.clf()\n","    plt.plot(fpr_list, tpr_list, lw=1, label='ROC (AUC = %0.2f)' % roc_auc)\n","    plt.plot([0, 1], [0, 1], '--', color='gray', label='Random Guessing')\n","    plt.xlim([-0.05, 1.05])\n","    plt.ylim([-0.05, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Receiver Operating Characteristic (ROC) Curve')\n","    plt.legend()\n","    result_path = result_path + '/roc_curve_record'\n","    plt.savefig(os.path.join(result_path,'Roc_curve'+str(fold_nums + 1)+'.png'))\n","\n","\n","# 輸出每一次 epoch 的結果\n","def CSV_Output(result_path,parameter,num_epochs,train_loss_list,train_acc_list,val_loss_list,val_acc_list,precision_list,recall_list,TPR_list,FPR_list,f1_score_list,fold_wrong_predict,fold_nums):\n","    # Create the 'csv_record' directory if it doesn't exist\n","    csv_record_dir = os.path.join(result_path, 'csv_record')\n","    if not os.path.exists(csv_record_dir):\n","        os.makedirs(csv_record_dir)\n","    with open(result_path + '/csv_record/epoch_fold'+str(fold_nums + 1)+'.csv','w',newline='') as file:\n","        writer = csv.writer(file)\n","        writer.writerow(['num_epochs','batch_size','learning_rate','num_classes','device','start_time','end_time','num_fold'])\n","        writer.writerow(parameter)\n","        writer.writerow('')\n","        writer.writerow(['Epoch','train_loss','train_acc','val_loss','val_acc','precision','recall','TPR','FPR','F1 score'])\n","        for epoch in range(num_epochs):\n","            writer.writerow([epoch + 1,\n","                            round(train_loss_list[epoch], 4),\n","                            round(train_acc_list[epoch].item(), 4),\n","                            round(val_loss_list[epoch], 4),\n","                            round(val_acc_list[epoch].item(), 4),\n","                            round(precision_list[epoch], 4),\n","                            round(recall_list[epoch], 4),\n","                            round(TPR_list[epoch], 4),\n","                            round(FPR_list[epoch], 4),\n","                            round(f1_score_list[epoch], 4)])\n","        writer.writerow([f'wrong_predict:',fold_wrong_predict])\n","    print('CSV output Sucessfully')\n","\n","def print_filename_in_txt(train_loader, valid_loader, result_path, fold_num):\n","    # Initialize empty lists to store filenames\n","    train_filenames = []\n","    valid_filenames = []\n","\n","    # Iterate over the training dataset\n","    for batch_idx, (data, target, filename) in enumerate(train_loader):\n","        # Append the filename to the list\n","        train_filenames.extend(filename)\n","\n","    # Iterate over the validation dataset\n","    for batch_idx, (data, target, filename) in enumerate(valid_loader):\n","        # Append the filename to the list\n","        valid_filenames.extend(filename)\n","\n","    # Create the 'file_name_record' directory if it doesn't exist\n","    file_record_dir = os.path.join(result_path, 'file_name_record')\n","    if not os.path.exists(file_record_dir):\n","        os.makedirs(file_record_dir)\n","\n","    # Save the training filenames as a text file\n","    with open(result_path + '/file_name_record/train_filenames_fold_'+str(fold_num + 1)+'.txt', 'w') as file:\n","        for filename in train_filenames:\n","            file.write(filename + '\\n')\n","\n","    # Save the validation filenames as a text file\n","    with open(result_path + '/file_name_record/valid_filenames_fold_'+str(fold_num + 1)+'.txt', 'w') as file:\n","        for filename in valid_filenames:\n","            file.write(filename + '\\n')\n","\n","    print(f\"Filenames saved in {result_path}/train_filenames.txt and {result_path}/valid_filenames.txt\")\n","\n","def calculate_average(avg_train_acc_list, avg_val_acc_list, avg_recall_list, avg_precision_list, avg_f1_score_list, avg_TPR_list, avg_FPR_list, fold_nums, result_path):\n","    # Calculate averages\n","    avg_train_acc = sum(avg_train_acc_list) / fold_nums\n","    avg_val_acc = sum(avg_val_acc_list) / fold_nums\n","    avg_recall = sum(avg_recall_list) / fold_nums\n","    avg_precision = sum(avg_precision_list) / fold_nums\n","    avg_f1_score = sum(avg_f1_score_list) / fold_nums\n","    avg_TPR = sum(avg_TPR_list) / fold_nums\n","    avg_FPR = sum(avg_FPR_list) / fold_nums\n","\n","    # Save averages to a text file\n","    with open(result_path + '/Average.txt', 'w') as file:\n","        file.write(f\"Avg Train Accuracy: {avg_train_acc}\\n\")\n","        file.write(f\"Avg Validation Accuracy: {avg_val_acc}\\n\")\n","        file.write(f\"Avg Recall: {avg_recall}\\n\")\n","        file.write(f\"Avg Precision: {avg_precision}\\n\")\n","        file.write(f\"Avg F1 Score: {avg_f1_score}\\n\")\n","        file.write(f\"Avg TPR: {avg_TPR}\\n\")\n","        file.write(f\"Avg FPR: {avg_FPR}\\n\")\n","\n","    print(f\"Averages saved to {result_path}\")\n","\n","def train(normal_data_dir,abnormal_data_dir,train_normal_tensor_path,train_abnormal_tensor_path,result_path,num_fold):\n","    #超參數設定\n","    batch_size = 52\n","    learning_rate = 0.001\n","    num_epochs = 50\n","    num_classes = 2\n","    num_folds = num_fold\n","    start_time = datetime.now()\n","    end_time = 0\n","    num_argumentation = 20\n","    conf_matrix = np.zeros((num_classes, num_classes), dtype=np.int32)\n","\n","    # Get the dataset\n","    train_normal_dataset = MyDataset(train_normal_tensor_path)\n","    train_abnormal_dataset = MyDataset(train_abnormal_tensor_path)\n","    # Combine the datasets\n","    train_datasets = CombinedDataset(train_normal_dataset,train_abnormal_dataset)\n","\n","    # Create the k-fold cross-validation object\n","    kfold = KFold(n_splits=num_folds)\n","\n","    #印出資料集大小\n","    print(\"train dataset's size : \" + str(len(train_datasets)))\n","\n","    #創建模型\n","    # model = models.resnet152(pretrained=True)\n","    model = timm.create_model('seresnet152d', pretrained=True)\n","    model.fc = nn.Linear(model.fc.in_features, num_classes)\n","    #print(model)\n","    #print(model.fc)\n","\n","    #將模型移動到GPU上進行運算\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model.to(device)\n","    model.fc.to(device)\n","    print(\"Device used : \" + str(device))\n","\n","    # make dir to save the training outcome\n","    result_path = mkdir_outcome(result_path)\n","\n","    avg_train_acc_list = []\n","    avg_val_acc_list = []\n","    avg_recall_list = []\n","    avg_precision_list = []\n","    avg_f1_score_list = []\n","    avg_TPR_list = []\n","    avg_FPR_list = []\n","\n","    files = []\n","    file_names = os.listdir(normal_data_dir)\n","    for file_name in file_names:\n","        name = os.path.splitext(file_name)[0]\n","        files.append(name)\n","\n","    file_names = os.listdir(abnormal_data_dir)\n","    for file_name in file_names:\n","        name = os.path.splitext(file_name)[0]\n","        files.append(name)\n","\n","    random.shuffle(files)\n","\n","    torch.save(model.state_dict(),os.path.join(result_path,\"seresnet152_pre_train.pt\"))\n","\n","    for fold, (train_indices, valid_indices) in enumerate(kfold.split(files)):\n","        print(f\"Fold: {fold+1}\")\n","\n","        #定義損失函數和優化器\n","        m = nn.Sigmoid()\n","        criterion = nn.BCELoss()\n","        # criterion = nn.CrossEntropyLoss()\n","        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n","\n","        train_loss_list = []\n","        train_acc_list = []\n","        val_loss_list = []\n","        val_acc_list = []\n","        recall_list = []\n","        precision_list = []\n","        f1_score_list = []\n","        TPR_list = []\n","        FPR_list = []\n","        fold_wrong_predict = []\n","\n","        train_files = [files[i] for i in train_indices]\n","        valid_files = [files[i] for i in valid_indices]\n","\n","       # 定義用於儲存訓練集和驗證集的檔案名稱的列表\n","        train_data_files = []\n","        valid_data_files = []\n","\n","        # 尋找所有訓練集資料檔名，根據增強資料的命名規則，找到對應的增強檔名\n","        for file in train_files:\n","            labels, index = file.split('_')\n","            # 建構增強後資料的檔名\n","            for num in range(1,num_argumentation + 1):\n","              augmented_file = f\"{labels}_{index}_aug{num}.pt\"\n","              # 將增強後資料的檔案名稱新增至訓練集檔案名稱列表\n","              train_data_files.append(augmented_file)\n","\n","        # 尋找所有驗證集資料檔名，根據增強資料的命名規則，找到對應的增強檔名\n","        for file in valid_files:\n","            labels, index = file.split('_')\n","            # 建構增強後資料的檔名\n","            for num in range(1,num_argumentation + 1):\n","              augmented_file = f\"{labels}_{index}_aug{num}.pt\"\n","              valid_data_files.append(augmented_file)\n","\n","        train = []\n","        valid = []\n","\n","        # 尋找所有combined_dataset中的樣本索引\n","        for index, (data, labels, filename) in enumerate(train_datasets):\n","            # 檢查目前樣本的檔案名稱是否在訓練集檔案名稱清單中\n","            if filename in train_data_files:\n","                train.append(index)\n","            # 檢查目前樣本的檔案名稱是否在驗證集檔案名稱清單中\n","            elif filename in valid_data_files:\n","                valid.append(index)\n","\n","        # Create the train and validation datasets for this fold\n","        train_dataset = torch.utils.data.Subset(train_datasets, train)\n","        valid_dataset = torch.utils.data.Subset(train_datasets, valid)\n","\n","\n","        print(\"train_dataset : \" + str(len(train_dataset)),\",valid_dataset : \" + str(len(valid_dataset)))\n","\n","        # Create the data loaders for this fold\n","        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n","        valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=True, num_workers=4)\n","        print_filename_in_txt(train_loader,valid_loader,result_path,fold)\n","\n","        #訓練模型\n","        for epoch in range(num_epochs):\n","            print(\"epoch:\"+f\"{epoch+1}\")\n","            train_loss = 0\n","            train_correct = 0\n","            train_acc = 0\n","            val_loss = 0\n","            val_corrects = 0\n","            val_acc = 0\n","\n","            print(\"result_path:\" + str(result_path))\n","\n","            if(epoch == 0) :\n","              model.load_state_dict(torch.load(os.path.join(result_path,\"seresnet152_pre_train.pt\")))\n","              print(\"Model initialized\")\n","            else :\n","              model.load_state_dict(torch.load(os.path.join(result_path,\"train\"+f'_fold_{fold+1}'+\".pt\")))\n","              print(\"Model loaded\")\n","\n","            #初始化\n","            conf_matrix = np.zeros((num_classes, num_classes), dtype=np.int32)\n","\n","            print(\"[Training Progress]: \")\n","            model.train()\n","            for inputs, labels, filename in tqdm(train_loader):\n","                targets=torch.eye(2)[labels.long(), :]\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","                targets = targets.to(device)\n","\n","                optimizer.zero_grad()\n","                outputs = model(inputs)\n","                loss = criterion(m(outputs),targets.float())\n","                _, preds = torch.max(outputs, 1)\n","                train_correct += torch.sum(preds == labels.data)\n","                loss.backward()\n","                optimizer.step()\n","                train_loss += loss.item() * inputs.size(0)\n","            train_loss = train_loss / len(train_loader.dataset)\n","            train_acc = train_correct.double() / len(train_loader.dataset)\n","            train_loss_list.append(train_loss)\n","\n","            model.eval()\n","\n","            print(\"[Validating Progress]: \")\n","            wrong_predict = []\n","            for inputs, labels, filename in tqdm(valid_loader):\n","                targets=torch.eye(2)[labels.long(), :]\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","                targets = targets.to(device)\n","                with torch.set_grad_enabled(False):\n","                    outputs = model(inputs)\n","                    loss = criterion(m(outputs),targets.float())\n","                val_loss += loss.item() * inputs.size(0)\n","                _, preds = torch.max(outputs, 1)\n","                val_corrects += torch.sum(preds == labels.data)\n","\n","                # Count Confusion Matrix\n","                for t, p in zip(preds.view(-1), labels.view(-1)):\n","                    conf_matrix[t.long(), p.long()] += 1\n","                    if t != p:\n","                      wrong_predict.append(filename)\n","            val_loss = val_loss / len(valid_loader.dataset)\n","            val_acc = val_corrects.double() / len(valid_loader.dataset)\n","            val_loss_list.append(val_loss)\n","\n","            scheduler.step()\n","            end_time = datetime.now()\n","            print('\\nEpoch: [{}/{}]  train_loss: {:<.4f}  -  train_accuracy: {:<.4f} -  val_loss: {:<.4f}  -  val_accuracy: {:<.4f}  -  val_correct: {:<10}'.format(\n","                epoch+1, num_epochs, train_loss, train_acc, val_loss, val_acc, val_corrects))\n","            print('wrong predict : ' + str(wrong_predict))\n","            torch.save(model.state_dict(),os.path.join(result_path,\"train\"+f'_fold_{fold+1}'+\".pt\"))\n","            print(\"Model saved\")\n","\n","            # validation index (評估指標)\n","            precision,recall,f1_score,TPR,FPR = validation_index(conf_matrix)\n","\n","            # record the outcomes\n","            val_acc_list.append(val_acc),train_acc_list.append(train_acc),precision_list.append(precision),recall_list.append(recall)\n","            f1_score_list.append(f1_score),TPR_list.append(TPR),FPR_list.append(FPR)\n","\n","            if epoch + 1 == num_epochs:\n","              torch.cuda.empty_cache()\n","              avg_train_acc_list.append(train_acc),avg_val_acc_list.append(val_acc),avg_recall_list.append(recall),avg_precision_list.append(precision),avg_f1_score_list.append(f1_score),avg_TPR_list.append(TPR),avg_FPR_list.append(FPR),fold_wrong_predict.append(wrong_predict)\n","\n","        # function of confusion matrix param(folder path, matrix, test normal dataset length, test unnormal dataset length)\n","        Confusion_Matrix(result_path,conf_matrix,fold)\n","        # functioN to show ROC curve\n","        ROC_Curve(result_path,TPR_list,FPR_list,fold)\n","        # CSV visualization\n","        param = [num_epochs,batch_size,learning_rate,num_classes,device,start_time,end_time,fold]\n","        CSV_Output(result_path,param,num_epochs,train_loss_list,train_acc_list,val_loss_list,val_acc_list,precision_list,recall_list,TPR_list,FPR_list,f1_score_list,fold_wrong_predict,fold)\n","\n","    #calculate the average\n","    calculate_average(avg_train_acc_list, avg_val_acc_list, avg_recall_list, avg_precision_list, avg_f1_score_list, avg_TPR_list, avg_FPR_list, num_folds, result_path)\n","\n","    torch.cuda.empty_cache()\n","\n","\n","normal_data_dir = '/content/drive/My Drive/Deep_X_torch/splitted_dataset/train/normal'\n","abnormal_data_dir = '/content/drive/My Drive/Deep_X_torch/splitted_dataset/train/abnormal'\n","train_normal_tensor_path = '/content/drive/My Drive/Deep_X_torch/tensor/train/normal'\n","train_abnormal_tensor_path = '/content/drive/My Drive/Deep_X_torch/tensor/train/abnormal'\n","result_path = '/content/drive/My Drive/Deep_X_torch/result_seresnet152'\n","\n","train(normal_data_dir,abnormal_data_dir,train_normal_tensor_path,train_abnormal_tensor_path,result_path,5)"]},{"cell_type":"markdown","metadata":{"id":"0p-MalRCZLME"},"source":["#測試模型(ResNet)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iKRb2fuSZPoc"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import os\n","import numpy as np\n","import torch.optim as optim\n","import torchvision.models as models\n","import matplotlib.pyplot as plt\n","import csv\n","from torch.utils.data import DataLoader, Dataset\n","from torch.utils.data import ConcatDataset\n","from sklearn import datasets\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import confusion_matrix\n","from tqdm import tqdm\n","from torch.utils.tensorboard import SummaryWriter\n","from datetime import datetime\n","\n","class MyDataset(Dataset):\n","    def __init__(self, data_path):\n","        self.data_path = data_path\n","        self.class_to_idx = {'abnormal': 1, 'normal': 0}  # 定義類別名稱到類別索引的映射\n","        self.data = []\n","        self.filenames = []  # store filenames\n","        for filename in os.listdir(data_path):\n","            if filename.endswith('.pt'):\n","                tensor = torch.load(os.path.join(data_path, filename))\n","                if filename.split('_')[0] == 'normal':\n","                    label_idx = 0\n","                else:\n","                    label_idx = 1\n","                self.data.append((tensor, label_idx, filename))\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        tensor, labels, filename = self.data[index]\n","        return tensor, labels, filename\n","\n","class CombinedDataset(ConcatDataset):\n","    def __init__(self, dataset1, dataset2):\n","        super().__init__([dataset1, dataset2])\n","\n","    def __getitem__(self, index):\n","        return super().__getitem__(index)\n","\n","    def __len__(self):\n","        return super().__len__()\n","\n","# 找尋最新的訓練結果\n","def find_train_result_path(result_path):\n","    file_names = os.listdir(result_path)\n","    train_result_path = None\n","    num_max = 0\n","    for file_name in file_names:\n","        if file_name.startswith(\"result_\"):\n","            num_str = file_name.split(\"_\")[1]\n","            num = int(num_str)\n","            if num > num_max:\n","                num_max = num\n","\n","    result_path = os.path.join(result_path,\"result_{}\".format(num_max))\n","    train_result_path = os.path.join(result_path,\"train_{}\".format(num_max))\n","    return train_result_path\n","\n","\n","# 建立資料夾顯示訓練結果\n","def mkdir_outcome(result_path):\n","    file_names = os.listdir(result_path)\n","    num_max = 0\n","    for file_name in file_names:\n","        if file_name.startswith(\"result_\"):\n","            num_str = file_name.split(\"_\")[1]\n","            num = int(num_str)\n","            if(num > num_max):\n","                num_max = num\n","    # make folder for train result\n","    result_path = os.path.join(result_path,\"result_{}\".format(num_max))\n","    result_path_test = os.path.join(result_path,\"test_{}\".format(num_max))\n","    os.makedirs(result_path,exist_ok=True)\n","    os.makedirs(result_path_test,exist_ok=True)\n","    return result_path_test\n","\n","\n","# 模型評估指標\n","def test_index(conf_matrix):\n","    # Confusion Matrix to calculate [accuracy,precision,recall]\n","    precision = 0.0\n","    recall = 0.0\n","    f1_score = 0.0\n","    if((conf_matrix[0][0] + conf_matrix[0][1]) != 0):\n","        precision = conf_matrix[0][0] / (conf_matrix[0][0] + conf_matrix[0][1])\n","    if((conf_matrix[0][0] + conf_matrix[1][0]) != 0):\n","        recall = conf_matrix[0][0] / (conf_matrix[0][0] + conf_matrix[1][0])\n","    if((precision + recall) != 0):\n","        f1_score = 2*precision*recall / (precision + recall)\n","    TPR = recall\n","    FPR = conf_matrix[0][1] / (conf_matrix[0][1] + conf_matrix[1][1])\n","    print(\"\\t      Precision: {:<.4f}  -  Recall: {:<.4f}  -  F1 Score: {:<.4f}\".format(precision,recall,f1_score))\n","    return precision,recall,f1_score,TPR,FPR\n","\n","\n","# 混淆矩陣\n","def Confusion_Matrix(result_path,conf_matrix,fold):\n","    # Create the 'confusion_matrix_record' directory if it doesn't exist\n","    confusion_matrix_record_dir = os.path.join(result_path, 'confusion_matrix_record')\n","    plt.clf()\n","    if not os.path.exists(confusion_matrix_record_dir):\n","        os.makedirs(confusion_matrix_record_dir)\n","\n","    confusion_matrix = np.array([[conf_matrix[0][0], conf_matrix[0][1]], [conf_matrix[1][0], conf_matrix[1][1]]])\n","    print(\"Confusion matrix:\")\n","    print(conf_matrix)\n","    plt.imshow(confusion_matrix, cmap=plt.cm.Blues, interpolation='nearest')\n","    plt.colorbar()\n","\n","    # confusion matrix index 各個 index 的數值\n","    for i in range(2):\n","        for j in range(2):\n","            text_color = 'black' if confusion_matrix[i][j] < 0.5 * confusion_matrix.max() else 'white'\n","            plt.annotate(str(confusion_matrix[i][j]), xy=(j, i), ha='center', va='center', color=text_color)\n","    tick_marks = np.arange(2)\n","    plt.xticks(tick_marks, ['Positive', 'Negative'])\n","    plt.yticks(tick_marks, ['Positive', 'Negative'])\n","    plt.ylabel('Predicted Label')\n","    plt.xlabel('True Label')\n","    plt.title('Confusion Matrix')\n","    result_path = result_path + '/confusion_matrix_record'\n","    plt.savefig(os.path.join(result_path,'test_confusion_matrix_'+f'{fold}'+'.png'))\n","\n","\n","# ROC曲線\n","def ROC_Curve(result_path,tpr_list,fpr_list,fold):\n","    # Create the 'roc_curve_record' directory if it doesn't exist\n","    roc_curve_record_dir = os.path.join(result_path, 'roc_curve_record')\n","    if not os.path.exists(roc_curve_record_dir):\n","        os.makedirs(roc_curve_record_dir)\n","\n","    # 計算 AUC\n","    roc_auc = np.trapz(tpr_list, fpr_list)\n","\n","    # 繪製 ROC 曲線\n","    plt.clf()\n","    plt.plot(fpr_list, tpr_list, lw=1, label='ROC (AUC = %0.2f)' % roc_auc)\n","    plt.plot([0, 1], [0, 1], '--', color='gray', label='Random Guessing')\n","    plt.xlim([-0.05, 1.05])\n","    plt.ylim([-0.05, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Receiver Operating Characteristic (ROC) Curve')\n","    plt.legend()\n","    result_path = result_path + '/roc_curve_record'\n","    plt.savefig(os.path.join(result_path,'test_Roc_curve_'+f'{fold}'+'.png'))\n","\n","\n","# 輸出每一次 epoch 的結果\n","def CSV_Output(test_result_path,param,num_epochs,test_loss_list,test_acc_list,precision_list,recall_list,TPR_list,FPR_list,f1_score_list,final_wrong_predict,fold):\n","    # Create the 'csv_record' directory if it doesn't exist\n","    csv_record_dir = os.path.join(test_result_path, 'csv_record')\n","    if not os.path.exists(csv_record_dir):\n","        os.makedirs(csv_record_dir)\n","    with open(test_result_path + '/csv_record'+'/test_epoch_'+f'{fold}'+'.csv','w',newline='') as file:\n","        writer = csv.writer(file)\n","        writer.writerow(['num_epochs','batch_size','learning_rate','num_classes','device','start_time','end_time'])\n","        writer.writerow(param)\n","        writer.writerow('')\n","        writer.writerow(['Epoch','test_loss','test_acc','precision','recall','TPR','FPR','F1 score'])\n","        for epoch in range(num_epochs):\n","            writer.writerow([epoch + 1,\n","                            round(test_loss_list[epoch], 4),\n","                            round(test_acc_list[epoch].item(), 4),\n","                            round(precision_list[epoch], 4),\n","                            round(recall_list[epoch], 4),\n","                            round(TPR_list[epoch], 4),\n","                            round(FPR_list[epoch], 4),\n","                            round(f1_score_list[epoch], 4)])\n","        writer.writerow([f'wrong predict:',final_wrong_predict])\n","    print('CSV output Sucessfully')\n","\n","def test(test_normal_tensor_path,test_abnormal_tensor_path,result_path,fold):\n","    #超參數設定\n","    batch_size = 1\n","    learning_rate = 0.001\n","    num_epochs = 5\n","    num_classes = 2\n","    start_time = datetime.now()\n","    end_time = 0\n","    conf_matrix = np.zeros((num_classes, num_classes), dtype=np.int32)\n","\n","    # Get the dataset\n","    test_normal_dataset = MyDataset(test_normal_tensor_path)\n","    test_abnormal_dataset = MyDataset(test_abnormal_tensor_path)\n","\n","    # Combine the datasets\n","    test_dataset = CombinedDataset(test_normal_dataset,test_abnormal_dataset)\n","\n","    #印出資料集大小\n","    print(\"dataset's size : \" + str(len(test_dataset)))\n","\n","    #創建模型\n","    model = models.resnet152(pretrained=False)\n","    model.fc = nn.Linear(model.fc.in_features, num_classes)\n","    #print(model)\n","    #print(model.fc)\n","\n","    #將模型移動到GPU上進行運算\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model.to(device)\n","    model.fc.to(device)\n","    print(\"Device used : \" + str(device))\n","\n","    #定義損失函數和優化器\n","    m = nn.Sigmoid()\n","    criterion = nn.BCELoss()\n","    # criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n","\n","    test_loss_list = []\n","    test_acc_list = []\n","    recall_list = []\n","    precision_list = []\n","    f1_score_list = []\n","    TPR_list = []\n","    FPR_list = []\n","    wrong_predict = []\n","    final_wrong_predict = []\n","\n","    # make dir to save the training outcome\n","    test_result_path = mkdir_outcome(result_path)\n","    print('test_result_path : ' + str(test_result_path))\n","\n","    train_result_path = find_train_result_path(result_path)\n","    print('train_result_path : ' + str(train_result_path))\n","\n","    for folds in range(1,fold + 1):\n","\n","        print(\"train model:\"+\"train_fold_\"+f'{folds}'+\".pt\")\n","\n","        # 載入訓練好的模型參數\n","        model.load_state_dict(torch.load(os.path.join(train_result_path, \"train_fold_\"+f'{folds}'+\".pt\")))\n","        test_loader = DataLoader(test_dataset, batch_size, shuffle=True, num_workers=4)\n","\n","        #測試模型\n","        for epoch in range(num_epochs):\n","            test_loss = 0\n","            test_corrects = 0\n","            test_acc = 0\n","            model.load_state_dict(torch.load(os.path.join(train_result_path, \"train_fold_\"+f'{folds}'+\".pt\")))\n","            model.eval()\n","\n","            #初始化\n","            conf_matrix = np.zeros((num_classes, num_classes), dtype=np.int32)\n","\n","            print(\"[Test Progress]: \")\n","            for inputs, labels, filename in tqdm(test_loader):\n","                targets=torch.eye(2)[labels.long(), :]\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","                targets = targets.to(device)\n","                with torch.set_grad_enabled(False):\n","                    outputs = model(inputs.to(device))\n","                    loss = criterion(m(outputs),targets.float())\n","                test_loss += loss.item() * inputs.size(0)\n","                _, preds = torch.max(outputs, 1)\n","                test_corrects += torch.sum(preds == labels.data)\n","\n","                # Count Confusion Matrix\n","                for t, p in zip(preds.view(-1), labels.view(-1)):\n","                    conf_matrix[t.long(), p.long()] += 1\n","                    if t != p:\n","                      wrong_predict.append(filename)\n","            test_loss =  test_loss / len(test_loader.dataset)\n","            test_acc =  test_corrects.double() / len(test_loader.dataset)\n","            test_loss_list.append(test_loss)\n","\n","            scheduler.step()\n","            end_time = datetime.now()\n","            print('\\nEpoch: [{}/{}]  test_loss: {:<.4f}  -  test_accuracy: {:<.4f}  -  test_correct: {:<10}'.format(\n","                epoch+1, num_epochs, test_loss, test_acc, test_corrects))\n","            print('wrong predicts : ' + str(wrong_predict))\n","\n","\n","            # teat index (評估指標)\n","            precision,recall,f1_score,TPR,FPR = test_index(conf_matrix)\n","\n","            # record the outcomes\n","            test_acc_list.append(test_acc),precision_list.append(precision),recall_list.append(recall)\n","            f1_score_list.append(f1_score),TPR_list.append(TPR),FPR_list.append(FPR)\n","\n","            if epoch + 1 == num_epochs:\n","              final_wrong_predict.append(wrong_predict)\n","            else:\n","              wrong_predict.clear()\n","\n","        # function of confusion matrix param(folder path, matrix, test normal dataset length, test unnormal dataset length)\n","        Confusion_Matrix(test_result_path,conf_matrix,folds)\n","        # functio to show ROC curve\n","        ROC_Curve(test_result_path,TPR_list,FPR_list,folds)\n","        # CSV visualization\n","        param = [num_epochs,batch_size,learning_rate,num_classes,device,start_time,end_time]\n","        CSV_Output(test_result_path,param,num_epochs,test_loss_list,test_acc_list,precision_list,recall_list,TPR_list,FPR_list,f1_score_list,final_wrong_predict,folds)\n","\n","        confusion_matrix = np.array([[conf_matrix[0][0], conf_matrix[0][1]], [conf_matrix[1][0], conf_matrix[1][1]]])\n","        print(\"Confusion matrix:\")\n","        print(conf_matrix)\n","        plt.clf()\n","        plt.imshow(confusion_matrix, cmap=plt.cm.Blues, interpolation='nearest')\n","        plt.colorbar()\n","        tick_marks = np.arange(2)\n","        plt.xticks(tick_marks, ['Positive', 'Negative'])\n","        plt.yticks(tick_marks, ['Positive', 'Negative'])\n","        plt.ylabel('Predicted Label')\n","        plt.xlabel('True Label')\n","        plt.title('Confusion Matrix({})'.format(len(test_dataset)))\n","        plt.show()\n","\n","        torch.cuda.empty_cache()\n","\n","\n","test_normal_tensor_path = '/content/drive/My Drive/Deep_X_torch/tensor/test/normal'\n","test_abnormal_tensor_path = '/content/drive/My Drive/Deep_X_torch/tensor/test/abnormal'\n","result_path = '/content/drive/My Drive/Deep_X_torch/result_resnet152'\n","\n","test(test_normal_tensor_path,test_abnormal_tensor_path,result_path,5)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"EnWTcuTMlr5O"},"source":["#測試模型(SEResNet)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ceO70P1ms0UX"},"outputs":[],"source":["!pip install timm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Ow5kdZGl2hn"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import os\n","import numpy as np\n","import torch.optim as optim\n","import torchvision.models as models\n","import matplotlib.pyplot as plt\n","import csv\n","import timm\n","from torch.utils.data import DataLoader, Dataset\n","from torch.utils.data import ConcatDataset\n","from sklearn import datasets\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import confusion_matrix\n","from tqdm import tqdm\n","from torch.utils.tensorboard import SummaryWriter\n","from datetime import datetime\n","\n","class MyDataset(Dataset):\n","    def __init__(self, data_path):\n","        self.data_path = data_path\n","        self.class_to_idx = {'abnormal': 1, 'normal': 0}  # 定義類別名稱到類別索引的映射\n","        self.data = []\n","        self.filenames = []  # store filenames\n","        for filename in os.listdir(data_path):\n","            if filename.endswith('.pt'):\n","                tensor = torch.load(os.path.join(data_path, filename))\n","                if filename.split('_')[0] == 'normal':\n","                    label_idx = 0\n","                else:\n","                    label_idx = 1\n","                self.data.append((tensor, label_idx, filename))\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        tensor, labels, filename = self.data[index]\n","        return tensor, labels, filename\n","\n","class CombinedDataset(ConcatDataset):\n","    def __init__(self, dataset1, dataset2):\n","        super().__init__([dataset1, dataset2])\n","\n","    def __getitem__(self, index):\n","        return super().__getitem__(index)\n","\n","    def __len__(self):\n","        return super().__len__()\n","\n","# 找尋最新的訓練結果\n","def find_train_result_path(result_path):\n","    file_names = os.listdir(result_path)\n","    train_result_path = None\n","    num_max = 0\n","    for file_name in file_names:\n","        if file_name.startswith(\"result_\"):\n","            num_str = file_name.split(\"_\")[1]\n","            num = int(num_str)\n","            if num > num_max:\n","                num_max = num\n","\n","    result_path = os.path.join(result_path,\"result_{}\".format(num_max))\n","    train_result_path = os.path.join(result_path,\"train_{}\".format(num_max))\n","    return train_result_path\n","\n","\n","# 建立資料夾顯示訓練結果\n","def mkdir_outcome(result_path):\n","    file_names = os.listdir(result_path)\n","    num_max = 0\n","    for file_name in file_names:\n","        if file_name.startswith(\"result_\"):\n","            num_str = file_name.split(\"_\")[1]\n","            num = int(num_str)\n","            if(num > num_max):\n","                num_max = num\n","    # make folder for train result\n","    result_path = os.path.join(result_path,\"result_{}\".format(num_max))\n","    result_path_test = os.path.join(result_path,\"test_{}\".format(num_max))\n","    os.makedirs(result_path,exist_ok=True)\n","    os.makedirs(result_path_test,exist_ok=True)\n","    return result_path_test\n","\n","\n","# 模型評估指標\n","def test_index(conf_matrix):\n","    # Confusion Matrix to calculate [accuracy,precision,recall]\n","    precision = 0.0\n","    recall = 0.0\n","    f1_score = 0.0\n","    if((conf_matrix[0][0] + conf_matrix[0][1]) != 0):\n","        precision = conf_matrix[0][0] / (conf_matrix[0][0] + conf_matrix[0][1])\n","    if((conf_matrix[0][0] + conf_matrix[1][0]) != 0):\n","        recall = conf_matrix[0][0] / (conf_matrix[0][0] + conf_matrix[1][0])\n","    if((precision + recall) != 0):\n","        f1_score = 2*precision*recall / (precision + recall)\n","    TPR = recall\n","    FPR = conf_matrix[0][1] / (conf_matrix[0][1] + conf_matrix[1][1])\n","    print(\"\\t      Precision: {:<.4f}  -  Recall: {:<.4f}  -  F1 Score: {:<.4f}\".format(precision,recall,f1_score))\n","    return precision,recall,f1_score,TPR,FPR\n","\n","\n","# 混淆矩陣\n","def Confusion_Matrix(result_path,conf_matrix,folds):\n","    # Create the 'confusion_matrix_record' directory if it doesn't exist\n","    confusion_matrix_record_dir = os.path.join(result_path, 'confusion_matrix_record'+'_fold'+f'{folds}')\n","    plt.clf()\n","    if not os.path.exists(confusion_matrix_record_dir):\n","        os.makedirs(confusion_matrix_record_dir)\n","\n","    confusion_matrix = np.array([[conf_matrix[0][0], conf_matrix[0][1]], [conf_matrix[1][0], conf_matrix[1][1]]])\n","    print(\"Confusion matrix:\")\n","    print(conf_matrix)\n","    plt.imshow(confusion_matrix, cmap=plt.cm.Blues, interpolation='nearest')\n","    plt.colorbar()\n","\n","    # confusion matrix index 各個 index 的數值\n","    for i in range(2):\n","        for j in range(2):\n","            text_color = 'black' if confusion_matrix[i][j] < 0.5 * confusion_matrix.max() else 'white'\n","            plt.annotate(str(confusion_matrix[i][j]), xy=(j, i), ha='center', va='center', color=text_color)\n","    tick_marks = np.arange(2)\n","    plt.xticks(tick_marks, ['Positive', 'Negative'])\n","    plt.yticks(tick_marks, ['Positive', 'Negative'])\n","    plt.ylabel('Predicted Label')\n","    plt.xlabel('True Label')\n","    plt.title('Confusion Matrix')\n","    result_path = result_path + '/confusion_matrix_record'+'_fold'+f'{folds}'\n","    plt.savefig(os.path.join(result_path,'test_confusion_matrix.png'))\n","\n","\n","# ROC曲線\n","def ROC_Curve(result_path,tpr_list,fpr_list,folds):\n","    # Create the 'roc_curve_record' directory if it doesn't exist\n","    roc_curve_record_dir = os.path.join(result_path, 'roc_curve_record'+'_fold'+f'{folds}')\n","    if not os.path.exists(roc_curve_record_dir):\n","        os.makedirs(roc_curve_record_dir)\n","\n","    # 計算 AUC\n","    roc_auc = np.trapz(tpr_list, fpr_list)\n","\n","    # 繪製 ROC 曲線\n","    plt.clf()\n","    plt.plot(fpr_list, tpr_list, lw=1, label='ROC (AUC = %0.2f)' % roc_auc)\n","    plt.plot([0, 1], [0, 1], '--', color='gray', label='Random Guessing')\n","    plt.xlim([-0.05, 1.05])\n","    plt.ylim([-0.05, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Receiver Operating Characteristic (ROC) Curve')\n","    plt.legend()\n","    result_path = result_path + '/roc_curve_record'+'_fold'+f'{folds}'\n","    plt.savefig(os.path.join(result_path,'test_Roc_curve.png'))\n","\n","\n","# 輸出每一次 epoch 的結果\n","def CSV_Output(test_result_path,param,num_epochs,test_loss_list,test_acc_list,precision_list,recall_list,TPR_list,FPR_list,f1_score_list,final_wrong_predict,folds):\n","    # Create the 'csv_record' directory if it doesn't exist\n","    csv_record_dir = os.path.join(test_result_path, 'csv_record'+'_fold'+f'{folds}')\n","    if not os.path.exists(csv_record_dir):\n","        os.makedirs(csv_record_dir)\n","    with open(csv_record_dir + '/test_epoch.csv','w',newline='') as file:\n","        writer = csv.writer(file)\n","        writer.writerow(['num_epochs','batch_size','learning_rate','num_classes','device','start_time','end_time'])\n","        writer.writerow(param)\n","        writer.writerow('')\n","        writer.writerow(['Epoch','test_loss','test_acc','precision','recall','TPR','FPR','F1 score'])\n","        for epoch in range(num_epochs):\n","            writer.writerow([epoch + 1,\n","                            round(test_loss_list[epoch], 4),\n","                            round(test_acc_list[epoch].item(), 4),\n","                            round(precision_list[epoch], 4),\n","                            round(recall_list[epoch], 4),\n","                            round(TPR_list[epoch], 4),\n","                            round(FPR_list[epoch], 4),\n","                            round(f1_score_list[epoch], 4)])\n","        writer.writerow([f'wrong predict:',final_wrong_predict])\n","    print('CSV output Sucessfully')\n","\n","def test(test_normal_tensor_path,test_abnormal_tensor_path,result_path,fold):\n","    #超參數設定\n","    batch_size = 1\n","    learning_rate = 0.001\n","    num_epochs = 5\n","    num_classes = 2\n","    start_time = datetime.now()\n","    end_time = 0\n","    conf_matrix = np.zeros((num_classes, num_classes), dtype=np.int32)\n","\n","    # Get the dataset\n","    test_normal_dataset = MyDataset(test_normal_tensor_path)\n","    test_abnormal_dataset = MyDataset(test_abnormal_tensor_path)\n","\n","    # Combine the datasets\n","    test_dataset = CombinedDataset(test_normal_dataset,test_abnormal_dataset)\n","\n","    #印出資料集大小\n","    print(\"dataset's size : \" + str(len(test_dataset)))\n","\n","    #創建模型\n","    # model = models.resnet152(pretrained=False)\n","    model = timm.create_model('seresnet152d', pretrained=False)\n","    model.fc = nn.Linear(model.fc.in_features, num_classes)\n","    #print(model)\n","    #print(model.fc)\n","\n","    #將模型移動到GPU上進行運算\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model.to(device)\n","    model.fc.to(device)\n","    print(\"Device used : \" + str(device))\n","\n","    #定義損失函數和優化器\n","    m = nn.Sigmoid()\n","    criterion = nn.BCELoss()\n","    # criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n","\n","    test_loss_list = []\n","    test_acc_list = []\n","    recall_list = []\n","    precision_list = []\n","    f1_score_list = []\n","    TPR_list = []\n","    FPR_list = []\n","    wrong_predict = []\n","    final_wrong_predict = []\n","\n","    # make dir to save the training outcome\n","    test_result_path = mkdir_outcome(result_path)\n","    print('test_result_path : ' + str(test_result_path))\n","\n","    train_result_path = find_train_result_path(result_path)\n","    print('train_result_path : ' + str(train_result_path))\n","\n","    for folds in range(1,fold + 1):\n","\n","        print(\"train model:\"+\"train\"+f'_fold_{folds}'+\".pt\")\n","\n","        # 載入訓練好的模型參數\n","        model.load_state_dict(torch.load(os.path.join(train_result_path, \"train\"+f'_fold_{folds}'+\".pt\")))\n","        test_loader = DataLoader(test_dataset, batch_size, shuffle=True, num_workers=4)\n","\n","        #測試模型\n","        for epoch in range(num_epochs):\n","            test_loss = 0\n","            test_corrects = 0\n","            test_acc = 0\n","            model.load_state_dict(torch.load(os.path.join(train_result_path, \"train\"+f'_fold_{folds}'+\".pt\")))\n","            model.eval()\n","\n","            #初始化\n","            conf_matrix = np.zeros((num_classes, num_classes), dtype=np.int32)\n","\n","            print(\"[Test Progress]: \")\n","            for inputs, labels, filename in tqdm(test_loader):\n","                targets=torch.eye(2)[labels.long(), :]\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","                targets = targets.to(device)\n","                with torch.set_grad_enabled(False):\n","                    outputs = model(inputs.to(device))\n","                    loss = criterion(m(outputs),targets.float())\n","                test_loss += loss.item() * inputs.size(0)\n","                _, preds = torch.max(outputs, 1)\n","                test_corrects += torch.sum(preds == labels.data)\n","\n","                # Count Confusion Matrix\n","                for t, p in zip(preds.view(-1), labels.view(-1)):\n","                    conf_matrix[t.long(), p.long()] += 1\n","                    if t != p:\n","                      wrong_predict.append(filename)\n","            test_loss =  test_loss / len(test_loader.dataset)\n","            test_acc =  test_corrects.double() / len(test_loader.dataset)\n","            test_loss_list.append(test_loss)\n","\n","            scheduler.step()\n","            end_time = datetime.now()\n","            print('\\nEpoch: [{}/{}]  test_loss: {:<.4f}  -  test_accuracy: {:<.4f}  -  test_correct: {:<10}'.format(\n","                epoch+1, num_epochs, test_loss, test_acc, test_corrects))\n","            print('wrong predicts : ' + str(wrong_predict))\n","\n","\n","            # teat index (評估指標)\n","            precision,recall,f1_score,TPR,FPR = test_index(conf_matrix)\n","\n","            # record the outcomes\n","            test_acc_list.append(test_acc),precision_list.append(precision),recall_list.append(recall)\n","            f1_score_list.append(f1_score),TPR_list.append(TPR),FPR_list.append(FPR)\n","\n","            if epoch + 1 == num_epochs:\n","              final_wrong_predict.append(wrong_predict)\n","            else:\n","              wrong_predict.clear()\n","\n","        # function of confusion matrix param(folder path, matrix, test normal dataset length, test unnormal dataset length)\n","        Confusion_Matrix(test_result_path,conf_matrix,folds)\n","        # functio to show ROC curve\n","        ROC_Curve(test_result_path,TPR_list,FPR_list,folds)\n","        # CSV visualization\n","        param = [num_epochs,batch_size,learning_rate,num_classes,device,start_time,end_time]\n","        CSV_Output(test_result_path,param,num_epochs,test_loss_list,test_acc_list,precision_list,recall_list,TPR_list,FPR_list,f1_score_list,final_wrong_predict,folds)\n","\n","        confusion_matrix = np.array([[conf_matrix[0][0], conf_matrix[0][1]], [conf_matrix[1][0], conf_matrix[1][1]]])\n","        print(\"Confusion matrix:\")\n","        print(conf_matrix)\n","        plt.clf()\n","        plt.imshow(confusion_matrix, cmap=plt.cm.Blues, interpolation='nearest')\n","        plt.colorbar()\n","        tick_marks = np.arange(2)\n","        plt.xticks(tick_marks, ['Positive', 'Negative'])\n","        plt.yticks(tick_marks, ['Positive', 'Negative'])\n","        plt.ylabel('Predicted Label')\n","        plt.xlabel('True Label')\n","        plt.title('Confusion Matrix({})'.format(len(test_dataset)))\n","        plt.show()\n","\n","        torch.cuda.empty_cache()\n","\n","\n","test_normal_tensor_path = '/content/drive/My Drive/Deep_X_torch/tensor/test/normal'\n","test_abnormal_tensor_path = '/content/drive/My Drive/Deep_X_torch/tensor/test/abnormal'\n","result_path = '/content/drive/My Drive/Deep_X_torch/result_seresnet152'\n","\n","test(test_normal_tensor_path,test_abnormal_tensor_path,result_path,5)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"634j9AKwr_FQ"},"source":["#清除資料"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OtldGDxYsDDk"},"outputs":[],"source":["import os\n","import torch\n","\n","def delete_file_under_path(path):\n","    for root, directories, files in os.walk(path):\n","        for file in files:\n","            os.remove(os.path.join(root, file))\n","\n","def delete_folder_under_path(path):\n","    for root, directories, files in os.walk(path, topdown=False):\n","        for directory in directories:\n","            folder_path = os.path.join(root, directory)\n","            os.rmdir(folder_path)\n","\n","normal_jpg_path = '/content/drive/My Drive/Deep_X_torch/original_dataset/normal/normal(.jpg)'\n","normal_label_path = '/content/drive/My Drive/Deep_X_torch/original_dataset/normal/normal_label'\n","abnormal_jpg_path = '/content/drive/My Drive/Deep_X_torch/original_dataset/abnormal/abnormal(.jpg)'\n","abnormal_label_path = '/content/drive/My Drive/Deep_X_torch/original_dataset/abnormal/abnormal_label'\n","processed_dir = '/content/drive/My Drive/Deep_X_torch/processed_dataset'\n","splitted_dir = '/content/drive/My Drive/Deep_X_torch/splitted_dataset'\n","tesor_path = '/content/drive/My Drive/Deep_X_torch/tensor'\n","seresnet_result_path = '/content/drive/My Drive/Deep_X_torch/result_seresnet152/'\n","resnet_result_path = '/content/drive/My Drive/Deep_X_torch/result_resnet152/'\n","all_data_dir = '/content/drive/My Drive/Deep_X_torch/all_data'\n","\n","torch.cuda.empty_cache()\n","\n","# delete_file_under_path(normal_jpg_path)\n","# delete_file_under_path(abnormal_jpg_path)\n","# delete_file_under_path(normal_label_path)\n","# delete_file_under_path(abnormal_label_path)\n","# delete_file_under_path(processed_dir)\n","# delete_file_under_path(splitted_dir)\n","# delete_file_under_path(all_data_dir)\n","# delete_file_under_path(tesor_path)\n","\n","# delete_file_under_path(seresnet_result_path)\n","# delete_folder_under_path(seresnet_result_path)\n","# delete_file_under_path(resnet_result_path)\n","# delete_folder_under_path(resnet_result_path)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["n8YuhhVNSL48","Pd2urWFtRs2f","hsQhWruTS-HN","ZnUUgJ0iOtxE","-qw_5dxUfMIJ","ZhJ0h0k8lRtk","EnWTcuTMlr5O","634j9AKwr_FQ"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}